{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6500656-9c56-478f-84f5-eec6d18d11ee",
   "metadata": {},
   "source": [
    "# This is the modeling notebook containing step by step process of upload various BERT (Bidirectional Encoder Representations from Transformers) LLMs available on huggingface.  This code comes from Subash Khanal, and this was used to persuade the usage of BERT base uncased for the model that classifies news as fake or real.  I included some transitions to help ease the reading of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577cb3a-b594-43f0-a1be-67edd8460712",
   "metadata": {},
   "source": [
    "# The workflow is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caaf4ea-4595-442d-beed-53ccc2c15497",
   "metadata": {},
   "source": [
    "# 1.) BERT Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec501620-07c7-4484-a422-c2d79d8ab1c7",
   "metadata": {},
   "source": [
    "# 2.) Optimizing BERT-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dd6f7-a982-479c-b519-eb3630512a37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3874b598-2d6c-40dc-ad84-834143e14721",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34621fca-3fcc-45a4-a160-da7aa4e452df",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a182183-8e95-4392-b9ef-9be0a3bb84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, log_loss, matthews_corrcoef, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b456d41-57d1-4613-9b05-99c8601a9f81",
   "metadata": {},
   "source": [
    "# 1.) BERT Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c968b5a-ed70-4493-8b48-845bea2950b1",
   "metadata": {},
   "source": [
    "# Load data and split it appropriately. Then convert the pandas dataframe to a hugging face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f50e49-004a-4dcb-bf20-e438cd6cf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('Final_Dataset_For_pipeling.csv')\n",
    "\n",
    "# Combine headline and description for model input\n",
    "data['text'] = data['Headlines'] + \" \" + data['description']\n",
    "data = data[['text', 'Target_final']]\n",
    "data.rename(columns={\"Target_final\": \"labels\"}, inplace=True)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = np.split(data.sample(frac=1, random_state=42), [int(.8*len(data))])\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fcfc6-d81b-405e-a22f-f1f3ee01d4fd",
   "metadata": {},
   "source": [
    "# The following hugging face BERTs were selected for testing: distilbert-base-uncased, albert-base-v2, robert-base, bert-base-uncased, microsoft/deberta-v3-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3dac0b-cc66-403a-9b34-aa9f6830a63c",
   "metadata": {},
   "source": [
    "# Initiate iteration to go over the various models with the following workflow:\n",
    "\n",
    "1.) Load tokenizer and model\n",
    "\n",
    "2.) Tokenize text\n",
    "\n",
    "3.) Convert tokenized text into pytorch ready format\n",
    "\n",
    "4.) Define the training parameters (learning rate = 2e-5, batch size = 16, number of train epochs = 2, weight decay = 0.01, GPU on)\n",
    "\n",
    "5.) Train model\n",
    "\n",
    "6.) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7c115-cda7-411c-80c9-a7372e3d172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to experiment with\n",
    "model_names = [\"distilbert-base-uncased\", \"albert-base-v2\", \"roberta-base\", \"bert-base-uncased\", \"microsoft/deberta-v3-small\"]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "    # Tokenize datasets\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluation metrics\n",
    "    def compute_metrics(predictions, labels):\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "        auc_roc = roc_auc_score(labels, predictions[:, 1])\n",
    "        logloss = log_loss(labels, predictions[:, 1])\n",
    "        mcc = matthews_corrcoef(labels, preds)\n",
    "\n",
    "        # Confusion matrix to get FPR and FNR\n",
    "        tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "        fp_rate = fp / (fp + tn)\n",
    "        fn_rate = fn / (fn + tp)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"auc_roc\": auc_roc,\n",
    "            \"log_loss\": logloss,\n",
    "            \"mcc\": mcc,\n",
    "            \"false_positive_rate\": fp_rate,\n",
    "            \"false_negative_rate\": fn_rate\n",
    "        }\n",
    "\n",
    "    # Evaluate model with progress tracking\n",
    "    print(\"Evaluating with progress tracking:\")\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for batch in tqdm(tokenized_test):\n",
    "        input_ids = batch['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = batch['attention_mask'].unsqueeze(0).to(device)\n",
    "        label = batch['labels'].item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = output.logits.cpu().numpy()\n",
    "            pred = torch.argmax(output.logits, dim=1).item()\n",
    "\n",
    "        predictions.append(logits)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert predictions to numpy array for metric calculations\n",
    "    predictions = np.vstack(predictions)\n",
    "\n",
    "    # Calculate metrics and store results\n",
    "    metrics = compute_metrics(predictions, labels)\n",
    "    results[model_name] = metrics\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Display final results for model comparison\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db83330-7aab-41e3-84ea-6e8994fc0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./best_bert_model\")\n",
    "tokenizer.save_pretrained(\"./best_bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cb942-9bd6-43c1-9276-02e1b25b62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the model directory\n",
    "!zip -r best_bert_model.zip best_bert_model\n",
    "\n",
    "# Download the zipped model file to your local machine\n",
    "from google.colab import files\n",
    "files.download(\"best_bert_model.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173de58a-adab-4c0c-8829-945f742a338c",
   "metadata": {},
   "source": [
    "# Best model was bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23466203-fb9a-44b4-9966-915c873d82d4",
   "metadata": {},
   "source": [
    "# 2.) Optimizing BERT-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed80f4-a2d6-411f-9923-7a54dc86f5c3",
   "metadata": {},
   "source": [
    "Optimize bert-base-uncased by balancing class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d878c-0f71-4165-91b9-1302e6d32fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization of bert-base-uncased model\n",
    "\n",
    "# Calculate class weights (optional, for imbalanced datasets)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "labels = [0, 1]  # Replace with actual labels\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7078db-5300-4375-9677-ebe2ba4fbc34",
   "metadata": {},
   "source": [
    "Training parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f144e5-be81-4156-a947-74ca61caf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments with optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    fp16=True,  # Mixed precision for GPU acceleration\n",
    "    gradient_accumulation_steps=2  # For larger effective batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472b3e4-71c5-448b-97ec-7da1d524f3dc",
   "metadata": {},
   "source": [
    "# Initiate a weighted trainer that uses the class weights determined above in the loss function (makes the lesser class, the real news, have higher weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ff682-3ea8-4bc0-b809-2b5d3b750664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer with weighted loss (if needed)\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3473e4-7a93-40d8-9086-5efa03418fde",
   "metadata": {},
   "source": [
    "# Go through the testing sequence again for the weight adjust BERT-base-uncased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18848b-5c77-4e60-9b7b-952be29ed821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8932c0-f84c-4655-acf1-a67f98603164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for bert-base-uncased\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27245f5-4b24-42b8-9a97-1e265fdcb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments with consistent save and evaluation strategies\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",         # Set evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",               # Set save strategy to match evaluation strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    fp16=True,                           # Enable mixed-precision training for GPU acceleration\n",
    "    gradient_accumulation_steps=2,       # Accumulate gradients to simulate larger batch size\n",
    "    save_total_limit=1,                  # Save only the best model\n",
    "    load_best_model_at_end=True          # Load the best model at the end\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293a8ef-7242-42d0-921a-8239c2567676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82750c-fd8c-478b-a811-d124bcf38ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Get predictions and true labels\n",
    "predictions, labels, _ = trainer.predict(eval_dataset)\n",
    "# Convert logits to class predictions\n",
    "preds = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7504b-7211-4218-ac42-ebbc1e6dda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "\n",
    "# AUC-ROC Score (needs probability scores for the positive class)\n",
    "auc_roc = roc_auc_score(labels, predictions[:, 1])\n",
    "\n",
    "# Log Loss (needs probability scores)\n",
    "logloss = log_loss(labels, softmax(torch.tensor(predictions), dim=1)[:, 1].numpy())\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(labels, preds)\n",
    "\n",
    "# Confusion Matrix to calculate False Positive and False Negative Rates\n",
    "tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "# Print each metric\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "print(f\"False Negative Rate: {false_negative_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4068e3-6013-4d4f-96a0-acfe00d9e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions on the validation set\n",
    "predictions, labels, _ = trainer.predict(eval_dataset)\n",
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()\n",
    "\n",
    "# Tune the threshold\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for threshold in np.arange(0.4, 0.6, 0.01):  # Tuning within a narrow range\n",
    "    adjusted_preds = (probs > threshold).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, adjusted_preds, average=\"binary\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Optimal Threshold: {best_threshold}, F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea95446-c157-4370-8049-26510ebda410",
   "metadata": {},
   "source": [
    "# Fine tune the model with a smaller learning rate (rate of weight change should decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc1257-8cd9-4243-ac5e-76b2a09e30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Fine-Tune with Smaller Learning Rate Decay\n",
    "\n",
    "# Define new training arguments with a smaller learning rate\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,  # Smaller learning rate for finer adjustments\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,  # Additional fine-tuning epochs\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True,  # Mixed precision for GPU acceleration\n",
    "    gradient_accumulation_steps=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Reinitialize Trainer with new training_args\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa772d-fe33-4b74-aeed-50d23e24ae7c",
   "metadata": {},
   "source": [
    "# Add dropout to increase the model's robustness with generalization (randomly removes weights of data in order to reduce overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9abf00-56d2-4933-b8bf-5f83397fc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Add Dropout Regularization\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the model with a custom dropout rate\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, hidden_dropout_prob=0.1)\n",
    "\n",
    "# Reinitialize Trainer with the updated model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine-tune the model again with dropout\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03e504-2507-43d5-8a65-b47f7eab0f12",
   "metadata": {},
   "source": [
    "# Apply focal loss so that the loss is more significant for smaller class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef539625-7d3b-486a-97dc-59e188d8456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Implement Focal Loss for Misclassification Handling\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        BCE_loss = F.cross_entropy(logits, labels, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = ((1 - pt) ** self.gamma) * BCE_loss\n",
    "        return torch.mean(F_loss)\n",
    "\n",
    "# Custom Trainer with Focal Loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fn = FocalLoss(gamma=2)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Use the custom trainer with focal loss\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine-tune the model with Focal Loss\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b50c0-3d54-496c-afc4-2f7b51dfb790",
   "metadata": {},
   "source": [
    "# Use cross validation for good practice of checking generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daf37b-d69d-48a3-9182-04815dd53cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Apply Cross-Validation to Validate Robustness\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "results = []\n",
    "\n",
    "# Cross-Validation loop\n",
    "for train_index, val_index in kfold.split(data):\n",
    "    train_data, val_data = data.iloc[train_index], data.iloc[val_index]\n",
    "\n",
    "    # Convert to Hugging Face Dataset format\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    eval_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "    # Apply tokenization to each fold\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Set format for PyTorch to include input_ids, attention_mask, and labels\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Initialize the trainer for each fold\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    results.append(metrics)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {metric: np.mean([fold[metric] for fold in results]) for metric in results[0]}\n",
    "print(f\"Cross-validated metrics: {avg_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db8991-cb1e-4599-9e6a-348b44853496",
   "metadata": {},
   "source": [
    "# Use early stopping in order to ensure not wasting computers resources if there is no decrease in loss aftter 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e44d10-38e9-4fc0-b9b3-2716e1d4a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Additional Fine-Tuning Epochs with Early Stopping\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Define training arguments with early stopping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer with EarlyStoppingCallback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]  # Stop if no improvement after 1 epoch\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b36670-6a59-4d18-aa9a-785b7d1c152a",
   "metadata": {},
   "source": [
    "# Evalutate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa904f-14c4-4acd-93df-a8e4097dc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and labels\n",
    "predictions, labels, _ = trainer.predict(eval_dataset)\n",
    "preds = np.argmax(predictions, axis=1)\n",
    "probs = softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "# Precision, Recall, F1 Score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "\n",
    "# AUC-ROC\n",
    "auc_roc = roc_auc_score(labels, probs[:, 1])\n",
    "\n",
    "# Log Loss\n",
    "logloss = log_loss(labels, probs[:, 1])\n",
    "\n",
    "# MCC (Matthews Correlation Coefficient)\n",
    "mcc = matthews_corrcoef(labels, preds)\n",
    "\n",
    "# Confusion Matrix to calculate False Positive and False Negative Rates\n",
    "tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "# Print each metric\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "print(f\"False Negative Rate: {false_negative_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d5895-5777-4fbc-b4c2-a886cf78a3e0",
   "metadata": {},
   "source": [
    "# Compare results between the original bert-base-uncased with the tuned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e061d-3823-4f16-87ee-022246b8db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing with the previous fine-tuned models evaluation metrices score\n",
    "comparison_results = {\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC-ROC\", \"Log Loss\", \"MCC\", \"False Positive Rate\", \"False Negative Rate\"],\n",
    "    \"Initial Model\": [0.8031, 0.7696, 0.8087, 0.7887, 0.8880, 0.4491, 0.6051, 0.2017,0.1913],  # Replace with initial values\n",
    "    \"Optimized Model\": [accuracy, precision, recall, f1, auc_roc, logloss, mcc, false_positive_rate, false_negative_rate]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b0f34-1173-453b-9977-f9107e248446",
   "metadata": {},
   "source": [
    "# save the model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d78ee8-9307-46d5-99b6-1aa9c8d82f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized model and tokenizer in a separate directory\n",
    "model.save_pretrained(\"./optimized_final_bert_model\")\n",
    "tokenizer.save_pretrained(\"./optimized_final_bert_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd31a42-6ed8-4964-b5dc-fccf5a864cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the optimized model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./optimized_final_bert_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c702c-00cf-4460-9209-171497684ee5",
   "metadata": {},
   "source": [
    "# Apply a quantization to the model in order to reduce the amount of computation power it consumes when running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61865d8-ebe1-410f-913e-64e635120389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Apply dynamic quantization to reduce model size and make it faster on CPU\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54717a-4116-4760-9496-c428649fcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model's state dictionary\n",
    "torch.save(quantized_model.state_dict(), \"./quantized_final_bert_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf145d1a-8238-4ade-bbc2-bb78bc1f3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized model structure\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"./optimized_final_bert_model\")\n",
    "\n",
    "# Apply dynamic quantization to match the saved quantized model structure\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    base_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Load the saved quantized weights\n",
    "quantized_model.load_state_dict(torch.load(\"./quantized_final_bert_model.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "quantized_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a959134-5832-442d-8a66-96aeb9f83ae8",
   "metadata": {},
   "source": [
    "# evaluate quantized version of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b66980-3fc4-4cdb-91eb-d9faa062a274",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate metrics of quantized model\n",
    "\n",
    "def evaluate_model(model, eval_dataset):\n",
    "    # Get predictions\n",
    "    predictions, labels, _ = trainer.predict(eval_dataset)\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    probs = softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    auc_roc = roc_auc_score(labels, probs[:, 1])\n",
    "    logloss = log_loss(labels, probs[:, 1])\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "\n",
    "    # Confusion matrix for FPR and FNR\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"MCC\": mcc,\n",
    "        \"False Positive Rate\": false_positive_rate,\n",
    "        \"False Negative Rate\": false_negative_rate\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aed10d-e628-433f-92e7-45d16c86af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting quantized model is in evaluation mode\n",
    "quantized_model.eval()\n",
    "\n",
    "# Evaluate and print metrics\n",
    "quantized_metrics = evaluate_model(quantized_model, eval_dataset)\n",
    "print(\"Quantized Model Metrics:\", quantized_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337e0c9-ffff-48cd-b319-ea7c20e2ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized model metrics for reference\n",
    "optimized_metrics = {\n",
    "    \"Accuracy\": 0.9588,\n",
    "    \"Precision\": 0.9416,\n",
    "    \"Recall\": 0.9971,\n",
    "    \"F1 Score\": 0.9685,\n",
    "    \"AUC-ROC\": 0.9942,\n",
    "    \"Log Loss\": 0.2073,\n",
    "    \"MCC\": 0.9123,\n",
    "    \"False Positive Rate\": 0.1079,\n",
    "    \"False Negative Rate\": 0.0029\n",
    "}\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nComparison of Optimized and Quantized Model Metrics:\")\n",
    "for metric, opt_value in optimized_metrics.items():\n",
    "    quant_value = quantized_metrics[metric]\n",
    "    print(f\"{metric}: Optimized = {opt_value:.4f}, Quantized = {quant_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1e3f1-ef43-4269-9dda-4e417b929c86",
   "metadata": {},
   "source": [
    "# save quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10767975-ab89-4e5c-ba84-c2d280125182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model's state dictionary\n",
    "torch.save(quantized_model.state_dict(), \"quantized_final_bert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c8380-4413-47e2-b653-d897791b1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Create a zip file and add the model file to it\n",
    "with zipfile.ZipFile(\"quantized_final_bert_model.zip\", \"w\") as zipf:\n",
    "    zipf.write(\"quantized_final_bert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab902a-b124-40db-8ee4-1e7fb3b2cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"quantized_final_bert_model.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548dc23d-46fe-4fe5-bc28-4797cb5723cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the zip file was created successfully\n",
    "!zipinfo quantized_final_bert_model.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505edb3b-ad6a-41e0-9a24-49eeba87a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the zip file\n",
    "files.download(\"quantized_final_bert_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923291f2-3e46-4e51-98dd-f139ae26f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(\"quantized_final_bert_model.zip\", \"r\") as zipf:\n",
    "    zipf.extractall()  # Extracts `quantized_final_bert_model.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd42fa-2182-496b-a80b-2c51e3f0a55c",
   "metadata": {},
   "source": [
    "# 3.) Final model code to go into Streamlit APp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6a2b9-fede-4317-befc-6398b4fd6759",
   "metadata": {},
   "source": [
    "# Lines of code to go into the Streamlit Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b08cd-3114-43d1-b4ac-5ffe798e8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the model structure\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    base_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Load the quantized weights\n",
    "quantized_model.load_state_dict(torch.load(\"quantized_final_bert_model.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "quantized_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6d495-77b7-4e4e-8f8e-f7d03264845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77e065-48a3-4ec9-8782-0e6750d6cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def predict_news(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = quantized_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Calculate confidence score\n",
    "        confidence = softmax(logits, dim=1).max().item() * 100  # Confidence in percentage\n",
    "        # Determine prediction label\n",
    "        prediction = \"real\" if logits.argmax() == 1 else \"fake\"\n",
    "\n",
    "    return prediction, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00563d88-424d-42e7-89b6-6b8bed510b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from user\n",
    "text_input = input(\"Enter a news headline or description: \")\n",
    "\n",
    "# Get the prediction and confidence score\n",
    "prediction, confidence = predict_news(text_input)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence: {confidence:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3b44f-c8eb-4827-8193-9551a99d0d31",
   "metadata": {},
   "source": [
    "The BERT-base-uncased was chosen as the best LLM to classify news as real or fake.  The model underwent several rounds of fine tuning in order to increase performance metrics as much as possible despite class imbalance issue causing generalization issues.  The model is ready for deployment onto Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7314638-21f9-4168-938e-de2261bc4f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
